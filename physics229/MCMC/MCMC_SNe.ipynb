{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov chain Monte Carlo\n",
    "\n",
    "Based on a project from Physics 151."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC chain\n",
    "\n",
    "When working in 'high' dimensional spaces it's often difficult to visualize likelihood surfaces, look at projections of the likelihood or even store the likelihood in a useful way.  This is made more complex if you want to compute the expectation value or distribution of some complex function of your parameters.\n",
    "\n",
    "A highly convenient way of handling all of these problems is a Markov Chain.  Such a chain consists of a sequence of values of your parameters (usually a file with one line of numbers for each $\\vec{\\theta}$) where the frequency in the chain is proportional to the probability of that model given the data.  With such a chain in hand many statistical problems become very straightforward.\n",
    "\n",
    "Markov Chains are generated by Markov Chain Monte Carlo algorithms: general methods based on drawing values of $\\theta$ from approximate distributions and then correcting those draws to better aproximate the target posterior distribution. The sampling is done sequentially, with the distribution of the sampled draws depending on the last value drawn - hence, the draws from a Markov chain. (p. 275, <i>Bayesian Data Analysis</i>, Andrew Gelman et al.; Remember that a sequence $x_1, x_2, ...$ of random events is called a Markov chain if $x_{n+1}$ depends explicitly only on $x_{n}$ and not on previous steps.)\n",
    "\n",
    "More advanced algorithms can operate to some extent in parallel (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Supernova Cosmology Project\n",
    "\n",
    "We will use a compilation of supernovae data to show that the expansion of the universe is accelerating, and hence it contains dark energy. This was [Nobel prize winning research](https://www.nobelprize.org/nobel_prizes/physics/laureates/2011/), and Saul Perlmutter, a professor of physics at Berkeley, shared a prize in 2011 for this discovery.  The method was deceptively simple:\n",
    "\n",
    "\"The expansion history of the universe can be determined quite easily, using as a “standard candle” any distinguishable class of astronomical objects of known intrinsic brightness that can be identified over a wide distance range. As the light from such beacons travels to Earth through an expanding universe, the cosmic expansion stretches not only the distances between galaxy clusters, but also the very wavelengths of the photons en route. By the time the light reaches us, the spectral wavelength $\\lambda$ has thus been redshifted by precisely the same incremental factor $z = \\Delta \\lambda/\\lambda$ by which the cosmos has been stretched in the time interval since the light left its source. The recorded redshift and brightness of each such object thus provide a measurement of the total integrated expansion of the universe since the time the light was emitted. A collection of such measurements, over a sufficient range of distances, would yield an entire historical record of the universe’s expansion.\" (Saul Perlmutter, http://supernova.lbl.gov/PhysicsTodayArticle.pdf).\n",
    "\n",
    "Type Ia Supernovae (which are thought to arise from the collapse of a white dwarf upon reaching the Chandrasekhar limit) are promising standard candles.\n",
    "\n",
    "We can infer the \"luminosity distance\" $D_L=(1+z)\\chi(z)$ from measuring the inferred brightness of a supernova of luminosity $L$.  In a flat Universe with matter density $\\Omega_m$\n",
    "$$\n",
    "\\chi(z) = c\\int_0^z \\frac{dz'}{H(z')}\n",
    "\\quad\\mathrm{with}\\quad\n",
    "H^2(z) = H_0^2\\left[\\Omega_m(1+z)^3 + (1-\\Omega_m)\\right]\n",
    "$$\n",
    "Writing $H_0 = 100\\, h\\ [\\mathrm{km}\\, s^{-1}\\mathrm{Mpc}^{-1}]$ the luminosity distance is\n",
    "$$\n",
    "  D_L = 2997.92458\\mathrm{Mpc}\\ \\,h^{-1}(1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)]^{1/2}}\n",
    "$$\n",
    "If we express fluxes in magnitudes $m$, where $m = -2.5\\mathrm{log}_{10}F$ + const, then distances can be written in terms of the distance modulus, $\\mu = m - M$ ($M$ is the absolute magnitude, the value of $m$ if the supernova is at a distance 10pc). Then:\n",
    "$$\n",
    "  \\mu = 25 + 5\\log_{10}\\left(\\frac{D_L}{\\mathrm{Mpc}}\\right)\n",
    "$$\n",
    "\n",
    "We will use the [SCP Union2.1 Supernova (SN) Ia compilation](http://supernova.lbl.gov/union/) to measure $\\Omega_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the measured data: z, $\\mu$ , $\\sigma(\\mu)$\n",
    "data = np.loadtxt(\"sn_z_mu_dmu_plow_union2.1.txt\", usecols=range(1,5))\n",
    "# z\n",
    "z_data = data[:,0]\n",
    "# mu\n",
    "mu_data = data[:,1]\n",
    "# error on mu (sigma(mu))\n",
    "mu_err_data = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and some models just as a sanity check.\n",
    "# We'll assume h=0.7 for now.\n",
    "def integrand_DL(z,Omegam):\n",
    "    intg = 1.0/np.sqrt(Omegam*(1+z)**3 + (1-Omegam))\n",
    "    return(intg)\n",
    "\n",
    "def DL_integrate(z,Omegam):\n",
    "    zval  = np.linspace(0.,z,1000)\n",
    "    DLval = 2997.92458/0.7*(1.0+z)*np.trapz(integrand_DL(zval,Omegam),x=zval)\n",
    "    return(DLval)\n",
    "\n",
    "def mu_model(z,Omegam):\n",
    "    mu_model = 25.0 + 5.0*np.log10(DL_integrate(z,Omegam))\n",
    "    return(mu_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z   = np.linspace(0.01, 1.5, 100)\n",
    "Mu1 = np.zeros(len(z))\n",
    "Mu2 = np.zeros(len(z))\n",
    "Mu3 = np.zeros(len(z))\n",
    "\n",
    "for i in range(len(z)):\n",
    "    Mu1[i] = mu_model(z[i],0)\n",
    "    \n",
    "for i in range(len(z)):\n",
    "    Mu2[i] = mu_model(z[i],0.3)\n",
    "\n",
    "for i in range(len(z)):\n",
    "    Mu3[i] = mu_model(z[i],1)\n",
    "    \n",
    "    \n",
    "plt.figure(figsize = (10,6))\n",
    "plt.errorbar(z_data, mu_data, yerr = mu_err_data, marker = 'o', mfc='crimson', mec='crimson', ecolor = 'crimson', elinewidth = 1.5, barsabove = True, capsize = 2.0,  linestyle = 'None', label = 'Data')\n",
    "plt.semilogx(z, Mu1, linewidth = 2.5, label = '$\\Omega_M = 0$')\n",
    "plt.semilogx(z, Mu2, linewidth = 2.5, label = '$\\Omega_M = 0.3$')\n",
    "plt.semilogx(z, Mu3, linewidth = 2.5, label = '$\\Omega_M = 1$')\n",
    "plt.legend()\n",
    "plt.xlim(0.01, 1.5)\n",
    "plt.xlabel('$z$',fontsize=18)\n",
    "plt.ylabel('$\\mu$',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've anticipated their great discovery already by plotting flat models with a cosmological constant above:\n",
    "\n",
    "\"If these data are correct, the obvious implication is that the simplest cosmological model must be too simple. The next simplest model might be one that Einstein entertained for a time. Believing the universe to be static, he tentatively introduced into the equations of general relativity an expansionary term he called the “cosmological constant” ($\\Lambda$) that would compete against gravitational collapse. After Hubble’s discovery of the cosmic expansion, Einstein famously rejected $\\Lambda$ as his “greatest blunder.” In later years, $\\Lambda$ came to be identified with the zero-point vacuum energy of all quantum fields. It turns out that invoking a cosmological constant allows us to fit the supernova data quite well.\" (Saul Perlmutter, https://www.nobelprize.org/nobel_prizes/physics/laureates/2011/)\n",
    "\n",
    "Rather than assuming DE is a cosmological constant let's allow it to have a general \"equation of state\", $w=p_{DE}/\\rho_{DE}$.  For simplicity we'll keep $w$ $z$-independent for now.  Then\n",
    "$$\n",
    "  H^2(z) = H_0^2[\\Omega_m(1+z)^3 + \\Omega_{DE}(1+z)^{3(1+w)} + (1-\\Omega_m-\\Omega_{DE})(1+z)^2]\n",
    "$$\n",
    "with the last term being curvature.  Taking $w = -1$ and $\\Omega_m+\\Omega_{DE}=1$ returns our earlier result.\n",
    "\n",
    "(For simplicity we'll assume the absolute magnitude of the SNe and $H_0$ are known, though one can sample from those as well if desired.  Keeping them fixed gives us a nice 2D problem for visualization though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize our above:\n",
    "def integrand_DL(z,Omegam,OmegaDE,w):\n",
    "    return (Omegam*(1+z)**3 + OmegaDE*(1+z)**(3*(1+w)) + (1-Omegam-OmegaDE)*(1+z)**2)**(-0.5)\n",
    "\n",
    "def DL_integrate(z,Omegam,OmegaDE,w):\n",
    "    zval  = np.linspace(0,z,1000)\n",
    "    DLval = 2997.92458/0.7*(1.0+z)*np.trapz(integrand_DL(zval,Omegam,OmegaDE,w),x=zval)\n",
    "    return DLval\n",
    "\n",
    "def mu_model(z,Omegam,OmegaDE,w):\n",
    "    mu_model = 25.0 + 5.0*np.log10(DL_integrate(z,Omegam,OmegaDE,w))\n",
    "    return mu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(0.01, 1.5, 100)\n",
    "Mu1 = np.zeros(len(z))\n",
    "Mu2 = np.zeros(len(z))\n",
    "Mu3 = np.zeros(len(z))\n",
    "\n",
    "for i in range(len(z)):\n",
    "    Mu1[i] = mu_model(z[i],0.3, 0, 0)\n",
    "    \n",
    "for i in range(len(z)):\n",
    "    Mu2[i] = mu_model(z[i],0, 1., -1)\n",
    "\n",
    "for i in range(len(z)):\n",
    "    Mu3[i] = mu_model(z[i],0.3, 0.7, -1)\n",
    "    \n",
    "plt.figure(figsize = (12,6))\n",
    "plt.errorbar(z_data, mu_data, yerr = mu_err_data, marker = 'o', mfc='crimson', mec='crimson', ecolor = 'crimson', elinewidth = 1.5, barsabove = True, capsize = 2.0,  linestyle = 'None', label = 'Data')\n",
    "plt.semilogx(z, Mu1, linewidth = 2.5, label = '$\\Omega_M = 0.3, \\Omega_{\\Lambda} = 0$')\n",
    "plt.semilogx(z, Mu2, linewidth = 2.5, label = '$\\Omega_M = 0, \\Omega_{\\Lambda} = 1$')\n",
    "plt.semilogx(z, Mu3, linewidth = 2.5, label = '$\\Omega_M = 0.3, \\Omega_{\\Lambda} = 0.7$')\n",
    "plt.legend()\n",
    "plt.xlim(0.01, 1.5)\n",
    "plt.xlabel('$z$',fontsize=18)\n",
    "plt.ylabel('$\\mu$',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially the \"money plot\" from the original SN paper (though with slightly updated data).\n",
    "\n",
    "The $\\Omega_m = 0.3$ and $\\Omega_m = 0.7$ model fits the data best. In combination with the CMB data, this shows that about 70% of the total energy density is vacuum energy and 30% is mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to a Bayesian MCMC analysis to estimate the cosmological parameters.\n",
    "\n",
    "Let us assume that the universe is flat (which is a fair assumption given modern data).\n",
    "Assuming that errors are Gaussian (can be justified by averaging over large numbers of SN; central limit theorem), we calculate the likelihood $L$ as:\n",
    "$$\n",
    "  L \\propto \\mathrm{exp}\\Big( -\\frac{1}{2} \\sum_{i = 1}^{N_{\\mathrm{SN}}} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m, w)]^2}{\\sigma(\\mu_i)^2} \\Big)\n",
    "$$\n",
    "where $z_i$, $\\mu_i$, $\\sigma(\\mu_i)$ are from the measurements, and we compute\n",
    "$\\mu_{model}$ as a function of $z$, $\\Omega_m$ and $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find the probability distribution of the data using the\n",
    "[Metropolis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm).\n",
    "\n",
    "1. Set the random initial point in the parameter space $(w, \\Omega_m)$: let $w$ be negative and $\\Omega_m$ be positive and draw a random number using np.random.uniform(). Set initial likelihood to low value (e.g. -1.e100) so that next point is accepted.\n",
    "\n",
    "2. Now, draw a new sample starting from this random initial point. Here we assume that the proposal distribution is Gaussian with arbitrary width: in this problem, we assume that $\\sigma = 0.01$ (This determines how far you propose jumps.) for distributions for both $w$ and $\\Omega_m$.\n",
    "\n",
    "3. Now, evaluate the log likelihood value of this new point.\n",
    "\n",
    "4. If the value has gone up, accept the point.\n",
    "\n",
    "5. Otherwise, accept it with probability given by ratio of likelihoods: Draw a random number from a uniform distribution between 0 and 1 ( $\\alpha$ = np.random.uniform() ). If the ratio $ln(\\frac{L_{new}}{L_{old}})$ is greater than $ln(\\alpha)$ (i.e. $\\frac{L_{new}}{L_{old}} > \\alpha$), then accept it. Otherwise, reject it and stay at your old point.\n",
    "\n",
    "6. Repeat this 15,000 times (the length of chain) and plot the distributions of $(w, \\Omega_m)$.\n",
    "\n",
    "I've arbitrarily set the length of MCMC chain to be 15,000. In the end, you should throw away the first 20% of the chain as burn-in. (20% is an arbitrary number). You can plot the chain and estimate the burn-in period and make the chain longer if necessary.\n",
    "\n",
    "__Note__ The implementation below is slow and painful, but is meant entirely for pedagogical reasons.  It takes a little while to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = np.loadtxt(\"sn_z_mu_dmu_plow_union2.1.txt\", usecols=range(1,5))\n",
    "z_data,mu_data,mu_err_data = data[:,0],data[:,1],data[:,2]\n",
    "\n",
    "# length of MCMC chain and number of parameters: Omega_m and w\n",
    "nsamples,npars = 15000,2\n",
    "\n",
    "# Define (gaussian) width of the proposal distribution, one for each parameter.\n",
    "# This determines how far you propose jumps\n",
    "Sigma = [0.01, 0.01]\n",
    "\n",
    "# Declare an empty array of the parameter values of each point. \n",
    "# Theta[:,0] stores a trace of the parameter \\Omega_m  \n",
    "# Theta[:,1] stores a trace of the parameter w \n",
    "# Theta[:,2] stores log-likelihood values at each point\n",
    "Theta = np.empty([nsamples,npars+1])\n",
    "\n",
    "# Random starting point in parameter space\n",
    "# Set initial likelihood to low value so next point is accepted (could compute it instead):\n",
    "Theta[0,:] = [np.random.uniform(), -np.random.uniform(), -1.e100]\n",
    "print(\"Initial params: \",Theta[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mu from theory\n",
    "# Really want something fast here, which can operate on vectors\n",
    "# of z's, so I'll use the fact that our integral turns out to\n",
    "# be expressable as Hypergeometric functions.  We could also\n",
    "# do power series expansions or Pade approximants or call some\n",
    "# other library.\n",
    "from scipy.special import hyp2f1\n",
    "def DL_z(z,OmegaM,w):\n",
    "    \"\"\"Returns D_L(z) for flat const-w models using Gaussian hypergeometric functions.\"\"\"\n",
    "    zp13w = (1+z)**(3*w)\n",
    "    fac   = (OmegaM-1)/OmegaM\n",
    "    a,b,c = 0.5,-1./6./w,1-1./6./w\n",
    "    t1    = hyp2f1(a,b,c,fac)\n",
    "    t2    = hyp2f1(a,b,c,fac*zp13w)\n",
    "    tmp   = (t1-t2/np.sqrt(1+z))\n",
    "    tmp   = 2997.925/0.7*(1+z) * 2/np.sqrt(OmegaM)*tmp\n",
    "    return(tmp)\n",
    "#\n",
    "def mu_model(z,Omegam,w):\n",
    "    mu_model = 25.0 + 5.0*np.log10(DL_z(z,Omegam,w))\n",
    "    return(mu_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the likelihood function:\n",
    "import math\n",
    "def lnL(Omegam,w):\n",
    "    # Treat unphysical regions by setting likelihood to (almost) zero:    \n",
    "    if(Omegam<=0 or w>=0):\n",
    "        lnL = -1.e100\n",
    "    else:\n",
    "        # Compute difference with theory mu at redshifts of the SN, for trial Omegam\n",
    "        Dmu = mu_data - mu_model(z_data,Omegam,w)\n",
    "        # Compute ln(likelihood) assuming gaussian errors\n",
    "        lnL = -0.5*np.sum((Dmu/mu_err_data)**2)\n",
    "    return lnL\n",
    "\n",
    "\n",
    "# Draw new proposed samples from a proposal distribution, centred on old values Omegam[i-1]\n",
    "# Accept or reject, and colour points according to ln(likelihood):\n",
    "\n",
    "# Compute initial likelihood value:\n",
    "Theta[0,npars] = lnL(Theta[0,0], Theta[0,1])\n",
    "print(\"Initial Likelihood\",Theta[0,npars])\n",
    "\n",
    "progress = nsamples/10; val = 0\n",
    "for i in range(1,nsamples):    \n",
    "    \n",
    "    if i%progress == 0:\n",
    "        val = val + 10\n",
    "        print(\"%d percent done\" %val)\n",
    "    \n",
    "    lnLPrevious = Theta[i-1,npars]\n",
    "    OmegamProp = np.random.normal(Theta[i-1,0],Sigma[0])\n",
    "    wProp = np.random.normal(Theta[i-1,1],Sigma[1])\n",
    "    \n",
    "    lnLProp    = lnL(OmegamProp, wProp)\n",
    "\n",
    "    # Metroplis-Hastings algorithm:\n",
    "\n",
    "    if(lnLProp > lnLPrevious):\n",
    "    # Accept point if likelihood has gone up:\n",
    "        Theta[i,0]     = OmegamProp\n",
    "        Theta[i,1]     = wProp\n",
    "        Theta[i,npars] = lnLProp\n",
    "    else:\n",
    "    # Otherwise accept it with probability given by ratio of likelihoods:\n",
    "        alpha = np.random.uniform()\n",
    "    \n",
    "        if(lnLProp - lnLPrevious > np.log(alpha)):\n",
    "            Theta[i,0]     = OmegamProp\n",
    "            Theta[i,1]     = wProp\n",
    "            Theta[i,npars] = lnLProp\n",
    "        else:\n",
    "        # Reject; Repeat the previous point in the chain:\n",
    "            Theta[i,0:2]     = Theta[i-1,0:2]\n",
    "            Theta[i,npars] = lnLPrevious\n",
    "\n",
    "# Remove a burn in period, arbitrarily chosen to be the first 20% of the chain:\n",
    "nburn = 2*math.floor(nsamples/10)\n",
    "    \n",
    "\n",
    "# Plot the histogram of Omegam after the burn-in phase:\n",
    "plt.hist(Theta[nburn:,0],bins=30)\n",
    "plt.xlabel(r'$\\Omega_m$',fontsize=18)\n",
    "plt.ylabel(r'Probability',fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "# Plot the histogram of w after the burn-in phase:\n",
    "plt.hist(Theta[nburn:,1],bins=30)\n",
    "plt.xlabel('w',fontsize=18)\n",
    "plt.ylabel(r'Probability',fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of the samples (2-d posterior):\n",
    "plt.scatter(Theta[nburn:,0], Theta[nburn:,1], c = -Theta[nburn:,npars])\n",
    "plt.xlabel(r'$\\Omega_m$',fontsize=18)\n",
    "plt.ylabel('w',fontsize=18)\n",
    "plt.show() \n",
    "\n",
    "# Print best-fit values and constraints\n",
    "print ('Omega_m = ',np.mean(Theta[nburn:,0]), '+/-' ,np.std(Theta[nburn:,0]))\n",
    "print ('w = ',np.mean(Theta[nburn:,1]), '+/-' ,np.std(Theta[nburn:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence ##\n",
    "\n",
    "So how do we know we've run enough samples?\n",
    "\n",
    "We need to make sure that chains converge to the posterior distribution. One useful test for convergence is \"Gelman-Rubin statistic.\" For a given parameter, $\\theta$, the $R$ statistic compares the variance across chains with the variance within a chain. Intuitively, if the chains are random-walking in very different places, i.e. not sampling the same distribution, $R$ will be large.\n",
    "\n",
    "In detail, given chains $J=1,\\ldots,m$, each of length $n$:\n",
    "* Let $B=\\frac{n}{m-1} \\sum_j \\left(\\bar{\\theta}_j - \\bar{\\theta}\\right)^2$, where $\\bar{\\theta_j}$ is the average $\\theta$ for chain $j$ and $\\bar{\\theta}$ is the global average. This is proportional to the variance of the individual-chain averages for $\\theta$.\n",
    "* Let $W=\\frac{1}{m}\\sum_j s_j^2$, where $s_j^2$ is the estimated variance of $\\theta$ within chain $j$. This is the average of the individual-chain variances for $\\theta$.\n",
    "* Let $V=\\frac{n-1}{n}W + \\frac{1}{n}B$. This is an estimate for the overall variance of $\\theta$.\n",
    "* Finally, $R=\\sqrt{\\frac{V}{W}}$.\n",
    "\n",
    "We'd like to see $R\\approx 1$ (e.g. $R < 1.1$ is often used). Note that this calculation can also be used to track convergence of combinations of parameters, or anything else derived from them. \n",
    "\n",
    "Reference: https://github.com/KIPAC/StatisticalMethods/blob/master/chunks/montecarlo1.ipynb\n",
    "\n",
    "Let us compute $R$ and determine if the condition $R < 1.1$ is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MCMC_chain():\n",
    "    # Make a new chain.\n",
    "    Theta = np.empty([nsamples,npars+1])\n",
    "    # Random starting point in parameter space\n",
    "    Theta[0,:] = [np.random.uniform(), -np.random.uniform(), -1.e100]\n",
    "    # Compute initial likelihood value:\n",
    "    Theta[0,npars] = lnL(Theta[0,0], Theta[0,1])\n",
    "    for i in range(1,nsamples):    \n",
    "        lnLPrevious = Theta[i-1,npars]\n",
    "        OmegamProp = np.random.normal(Theta[i-1,0],Sigma[0])\n",
    "        wProp = np.random.normal(Theta[i-1,1],Sigma[1])\n",
    "        lnLProp    = lnL(OmegamProp, wProp)\n",
    "        #\n",
    "        # Metroplis-Hastings algorithm:\n",
    "        if(lnLProp > lnLPrevious):\n",
    "        # Accept point if likelihood has gone up:\n",
    "            Theta[i,0]     = OmegamProp\n",
    "            Theta[i,1]     = wProp\n",
    "            Theta[i,npars] = lnLProp\n",
    "        else:\n",
    "        # Otherwise accept it with probability given by ratio of likelihoods:\n",
    "            alpha = np.random.uniform()\n",
    "    \n",
    "            if(lnLProp - lnLPrevious > np.log(alpha)):\n",
    "                Theta[i,0]     = OmegamProp\n",
    "                Theta[i,1]     = wProp\n",
    "                Theta[i,npars] = lnLProp\n",
    "            else:\n",
    "            # Reject; Repeat the previous point in the chain:\n",
    "                Theta[i,0:2]     = Theta[i-1,0:2]\n",
    "                Theta[i,npars] = lnLPrevious\n",
    "    # Remove a burn in period, arbitrarily chosen to be the first 20% of the chain:\n",
    "    nburn = 2*math.floor(nsamples/10)\n",
    "    return(Theta[nburn:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = create_MCMC_chain()\n",
    "chain2 = create_MCMC_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do OmegaM.\n",
    "n,m = chain1.shape[0],2\n",
    "OmM1= np.mean(chain1[:,0])\n",
    "OmM2= np.mean(chain2[:,0])\n",
    "OmM = np.mean([chain1[:,0],chain2[:,0]])\n",
    "B   = n/(m-1.0)*( (OmM1-OmM)**2+(OmM2-OmM)**2 )\n",
    "W   = np.mean([np.var(chain1[:,0]),np.var(chain2[:,0])])\n",
    "V   = (n-1.)/n*W + B/n\n",
    "R   = np.sqrt(V/W)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is safely less than 1.1, so we ran enough samples (we would probably combine those two chains into one).  Depending on how big the chains were, we could also \"thin\" them by taking every few samples rather than keeping all of the samples.  If you look at the chain you'll find it tends to have long streches of the same value (where nothing was accepted).  One way to see how \"long\" your chain is and how \"thinned\" it could be is to compare lengths to the \"correlation length\".\n",
    "\n",
    "The autocorrelation of a sequence, as a function of lag, $k$, is defined thusly:\n",
    "$$\n",
    "\\rho_k = \\frac{\\sum_{i=1}^{n-k}\\left(\\theta_{i} - \\bar{\\theta}\\right)\\left(\\theta_{i+k} - \\bar{\\theta}\\right)}{\\sum_{i=1}^{n-k}\\left(\\theta_{i} - \\bar{\\theta}\\right)^2} = \\frac{\\mathrm{Cov}_i\\left(\\theta_i,\\theta_{i+k}\\right)}{\\mathrm{Var}(\\theta)}\n",
    "$$\n",
    "The larger lag one needs to get a small autocorrelation, the less informative individual samples are.  The correlation length can be defined as how far apart samples have to be for the correlation to drop to a certain fraction of the zero-lag value.\n",
    "\n",
    "An alternative to the autocorrelation of the sequence is to plot its power spectrum (i.e. Fourier transform squared).  If the samples were uncorrelated the power spectrum would be flat (i.e. white noise).  So we expect to see a long, flat piece of P(k) and then less power at high k because the samples are highly correlated.  The length of the flat segment is basically how many \"independent\" samples you have in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1_ft = np.fft.fft(chain1[:,0]-np.mean(chain1[:,0]))[:100]\n",
    "chain1_Pk = np.abs(chain1_ft)**2\n",
    "plt.semilogx(chain1_Pk)\n",
    "plt.xlabel(r'$k/k_{\\rm fund}$',fontsize=18)\n",
    "plt.ylabel(r'$P(k)$',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have less than O(10) \"fully sampled\" pieces in this chain.  Long enough to be giving us a fair sample, but not overkill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canned packages ##\n",
    "\n",
    "It's not much work to write your own MCMC code, even using the more modern algorithms.  However, the cosmology community primarily uses two packages: [Cobaya](https://cobaya.readthedocs.io/en/latest/), which is usually integrated with CAMB or CLASS and has multiple likelihoods and data sets \"built in\", and [emcee](http://dfm.io/emcee/current/) which uses the more modern Goodman & Weare affine invariant ensemble sampler.  Personally I like the Goodman & Weare sampler because it (a) doesn't require lots of tuning of the proposal distribution and (b) allows modest amounts of parallelization (e.g. up to hundreds of \"walkers\").  Cobaya is in some form a replacement for the older [CosmoMC](https://cosmologist.info/cosmomc/), which is still used in a lot of legacy codes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
