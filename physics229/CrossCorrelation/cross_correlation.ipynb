{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlations with CMB lensing -- Example: BOSS x Planck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the usual packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Healpix ##\n",
    "\n",
    "While there are several ways of pixelizing the sphere (i.e. putting scalar or tensor data onto the sphere in a discrete way) the cosmology community almost uniformly uses the [Healpix](https://healpix.jpl.nasa.gov/) scheme.  Healpix is built around 12 base pixels (each about 1 steradian in area) which are then subdivided into 4, each of those in 4 again and so on.  You can see pictures in the documentation link earlier.  There are two advantages of Healpix for our purposes:\n",
    "\n",
    "1. Each pixel is approximately equal area, so integrals over the sphere become sums over pixels in a simple way.\n",
    "\n",
    "2. Pixels lie in iso-latitude rings, i.e. groups have the same $\\theta$ in spherical coordinates.  This means in the spherical harmonic transform, the $\\phi$ integral becomes an FFT (at fixed $\\theta$), and the $\\theta$ integral can be done using recurrence relations among the $P_\\ell^m$.\n",
    "\n",
    "We'll use the `Healpy` package for manipulating Healpix maps (more on other packages below).  You can install `Healpy` using pip, e.g. from within a notebook\n",
    "```\n",
    "!pip install healpy\n",
    "```\n",
    "or from the terminal just pip install.\n",
    "Beware that the Conda version uses obsolete libraries and this can cause \"issues\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are numerous, public libraries to perform analysis of\n",
    "# sky maps, including power spectra, correlation functions, etc.\n",
    "# Here we'll use a very basic version, the HealPy package which\n",
    "# handles Healpix maps [https://healpix.jpl.nasa.gov/].\n",
    "# (See later for alternative packages).\n",
    "#\n",
    "# !pip install healpy\n",
    "import healpy as hp\n",
    "#\n",
    "# Let's look at the 12 \"base\" pixels using a full-sky\n",
    "# Mollweide projection (as provided in the Healpy package).\n",
    "# We'll use the default, \"ring\" scheme.\n",
    "hp.mollview(np.arange(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's go to a higher \"Nside\".  Nside must be a power of 2.\n",
    "Nside = 4\n",
    "hp.mollview(np.arange(12*Nside**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with multiple options to mollview to rotate the map, overlay maps, change coordinates, put on various cuts, scales etc.  If you prefer other projections of the sky the package also contains a few other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The maps I've made below have N_side=1024, so set this here.\n",
    "Nside = 1024\n",
    "Npix  = 12*Nside**2 # Healpix map has 12 * N_side**2 pixels\n",
    "Lmax  = 3*Nside-1   # Maximum ell supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Cl analysis ##\n",
    "\n",
    "We will be computing the cross-correlation within the \"pseudo-Cl\" formalism, which is close to optimal at large ell.  Almost all CMB/LSS angular power spectrum analyses use a version of this method, sometimes in combination with a more optimal method at very low $\\ell$.\n",
    "\n",
    "The basic idea of the pseudo-$C_\\ell$ analysis is to pretend we had a full-sky map with no missing data or other oddities.  In this case computing the power spectrum or cross-spectrum would be trivial: take the $Y_{\\ell m}$ transform, square the coefficients and average the $2\\ell+1$ $m$-modes into a single $\\ell$-bin.  Step 1 of the pseudo-$C_\\ell$ analysis is to do this, calling our result $\\tilde{C}_\\ell$:\n",
    "$$\n",
    "  \\tilde{C}_\\ell = \\frac{1}{2\\ell+1}\\sum_{m} a_{\\ell m} a^\\star_{\\ell m}\n",
    "$$\n",
    "But what we have is not a full sky map but rather the map multiplied by a \"mask\".  So what we've computed here is not $C_\\ell$ but $\\tilde{C}_\\ell$, which is $C_\\ell$ convolved with a \"mode mixing matrix\", $M_{\\ell\\ell'}$.  It turns out this mode mixing matrix is purely geometrical, and it can be straightforwardly computed given the mask.  Suppose our mask, $W$, is $1$ where we have data and $0$ elsewhere.  Then $W^2=W$.  If we define\n",
    "$$\n",
    "  W_{\\ell\\ell' mm'} = \\int d\\hat{n}\\ W(\\hat{n})Y_{\\ell m}^\\star(\\hat{n})Y_{\\ell'm'}(\\hat{n})\n",
    "  = \\int_{\\rm obs} d\\hat{n}\\ Y_{\\ell m}^\\star(\\hat{n})Y_{\\ell'm'}(\\hat{n})\n",
    "$$\n",
    "then what we've computed is\n",
    "$$\n",
    "  \\tilde{a}_{\\ell m} = \\sum_{\\ell' m'} a_{\\ell' m'}^{\\rm true} W_{\\ell\\ell' mm'}\n",
    "  \\quad\\Rightarrow\\quad\n",
    "  \\left\\langle\\tilde{C}_\\ell\\right\\rangle = \\frac{1}{2\\ell+1}\\sum_{\\ell'} C_{\\ell'}\n",
    "  \\sum_{mm'} \\left|W_{\\ell\\ell' mm'}\\right|^2 = \\sum_{\\ell'} M_{\\ell\\ell'} C_{\\ell'}\n",
    "$$\n",
    "If we deconvolve this mask, we recover $C_\\ell$.\n",
    "\n",
    "Imagine $C_\\ell$ is very slowly varying on the scale where $W_{\\ell\\ell' mm'}$ is non-trivial.  Then\n",
    "\\begin{eqnarray}\n",
    "  \\left\\langle \\tilde{C}_\\ell\\right\\rangle &=& \\frac{1}{2\\ell+1}\\sum_{\\ell'} C_\\ell\n",
    "  \\sum_{mm'} \\left| W_{\\ell\\ell' mm'} \\right|^2 \\\\\n",
    "  &\\approx& \\frac{C_\\ell}{2\\ell+1}\\sum_{\\ell'mm'} \\int d\\hat{n}\\ W(\\hat{n})Y_{\\ell m}^\\star(\\hat{n})Y_{\\ell'm'}(\\hat{n})\n",
    "  \\int d\\hat{n}'\\ W(\\hat{n}')Y_{\\ell m}(\\hat{n}')Y_{\\ell'm'}^\\star(\\hat{n}') \\\\\n",
    "  &=& C_\\ell \\int d\\hat{n}\\ W^2(\\hat{n})\\ \\frac{1}{2\\ell+1}\\sum_{m} Y_{\\ell m}^\\star(\\hat{n})Y_{\\ell'm'}(\\hat{n}) \\\\\n",
    "  &=& C_\\ell \\int\\frac{d\\hat{n}}{4\\pi}\\ W(\\hat{n}) \\\\\n",
    "  &=& C_\\ell f_{\\rm sky}\n",
    "\\end{eqnarray}\n",
    "So the \"quick\" way to deconvolve the mask is simply to divide by $f_{\\rm sky}$!\n",
    "\n",
    "The lowest order effect of having incomplete sky coverage is that the number of modes per $\\ell$ is no longer $2\\ell+1$ but rather $(2\\ell+1)f_{\\rm sky}$ so that\n",
    "$$\n",
    "  {\\rm Cov}\\left[ C_\\ell , C_{\\ell'} \\right] = \\frac{1}{(2\\ell+1)f_{\\rm sky}}\n",
    "  \\left( C_\\ell^{\\rm sig}+C_\\ell^{\\rm noise} \\right)^2 \\delta_{\\ell\\ell'}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planck lensing map ###\n",
    "\n",
    "To save you some time I downloaded the Planck 2018 lensing map from the Planck Legacy Archive.  It comes packaged as a set of a_lm, but I converted it into a map using:\n",
    "\n",
    "```\n",
    "pl_lensing_alm = hp.read_alm('dat_klm.fits')       # Load alm file of kappa map\n",
    "pl_lensing = hp.alm2map(pl_lensing_alm, Nside)     # Make map for viewing.\n",
    "```\n",
    "\n",
    "The result is \"P18_lens_kap.fits\".  I also downgraded the Planck mask to Nside=1024 using the hp.ud_grade command.\n",
    "\n",
    "If you want to duplicate this, go to\n",
    "`https://pla.esac.esa.int/`\n",
    "then cosmology products, lensing products and download\n",
    "`COM_Lensing_4096_R3.00.tgz`\n",
    "Unpack it.  We need:\n",
    "* COM_Lensing_4096_R3.00/MV/dat_klm.fits\n",
    "* COM_Lensing_4096_R3.00/mask.fits.gz\n",
    "\n",
    "and you can delete the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_lensing = hp.read_map(\"P18_lens_kap.fits\")\n",
    "pl_mask = hp.read_map('P18_lens_msk.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a plot of this just to see what we've got.  Here it is at low resolution and with no filtering..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(hp.ud_grade(pl_lensing*pl_mask,256),min=-15,max=15,title='Planck $\\kappa$ map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a much nicer plot that this.  One thing we could do is to Wiener filter the map.  This amounts to multiplying all of the $a_{\\ell m}$ by $S/(S+N)$.  Along with lensing data the Planck team provides a noise power spectrum.  The lensing power spectrum can be computed either from a Boltzmann code or from the data itself so the filtering kernel is pretty straightforward.  We'll do something easier, since this is just for visualization purposes.  To avoid ringing due to a sharp $\\ell$-cut let's filter with\n",
    "$$\n",
    "  F_\\ell = \\frac{1}{2}\\left[ 1 - \\tanh\\left(\\frac{\\ell-\\ell_{\\rm max}}{\\sigma}\\right) \\right]\n",
    "$$\n",
    "where e.g. $\\sigma=20$ and we can choose $\\ell_{\\rm max}$ as we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our filtering kernel... the numbers below are\n",
    "# fairly arbitrary but make an okay looking figure.\n",
    "filt_ells = np.arange(Lmax)\n",
    "filt_sigm = 20.\n",
    "filt_lmax = 400.\n",
    "filt_vals = 0.5*(1-np.tanh((filt_ells-filt_lmax)/filt_sigm))\n",
    "#\n",
    "# Now filter the map ...\n",
    "unfl_alm = hp.map2alm(pl_lensing)\n",
    "filt_alm = hp.almxfl(unfl_alm,filt_vals)\n",
    "filt_map = hp.alm2map(filt_alm,256)\n",
    "# and plot it.  We'll use a \"masked array\" so that we\n",
    "# get the nice greyed out regions where we have masked\n",
    "# the sky. Note Healpix/Healpy use the 'inverse' of the\n",
    "# NumPy masked array convention, hence the \"1-\" below...\n",
    "m = hp.ma(filt_map)\n",
    "m.mask = 1-hp.ud_grade(pl_mask,256)\n",
    "hp.mollview(m.filled(),title=r'Planck $\\kappa$ map')\n",
    "hp.graticule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now read the galaxy data. ##\n",
    "\n",
    "To save you some time, I downloaded the BOSS DR12 galaxy catalog from\n",
    "\n",
    "`\n",
    "wget https://data.sdss.org/sas/dr12/boss/lss/galaxy_DR12v5_CMASSLOWZ_North.fits.gz\n",
    "`\n",
    "\n",
    "and then used the following to make a Healpix map of the number of galaxies per pixel:\n",
    "\n",
    "```\n",
    "from astropy.table import Table\n",
    "t = Table.read('galaxy_DR12v5_CMASSLOWZ_North.fits.gz')['RA','DEC']\n",
    "print(t[:5])\n",
    "# Make a histogram of the counts, converted from RA/DEC to galactic\n",
    "# coordinates.  This will be n_gal(\\hat{n}).\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "g = SkyCoord(ra=t['RA']*u.degree,dec=t['DEC']*u.degree,frame='fk5')\n",
    "g.transform_to('galactic')\n",
    "ipix = hp.ang2pix(Nside,g.galactic.l.value,g.galactic.b.value,lonlat=True)\n",
    "boss,b = np.histogram(ipix,bins=np.arange(12*Nside**2+1)-0.5)\n",
    "```\n",
    "\n",
    "This map is \"BOSS_DR12.fits\".  If you'd prefer to avoid the astropy routines you can also change coordinates using the Healpix \"rotator\" methods.  In our case we would rotate coordinates ```coord=['C','G']```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boss = hp.read_map('BOSS_DR12.fits')\n",
    "#\n",
    "# We want to make a mask for these data -- what we should REALLY\n",
    "# do is take their observing strategy and/or random catalog and\n",
    "# weights and make a mask from that.  To be really crude let's just\n",
    "# downgrade the map and pick non-zero pixels.\n",
    "boss_low = hp.ud_grade(boss,64)\n",
    "boss_mask = np.zeros_like(boss_low)\n",
    "boss_mask[boss_low>0]=1.0\n",
    "boss_mask = hp.ud_grade(boss_mask,Nside)\n",
    "print(\"Sky fraction {:f}\".format(np.sum(boss_mask)/len(boss_mask)))\n",
    "hp.mollview(hp.ud_grade(boss_mask,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mask is product of PLANCK lensing and CMASS masks\n",
    "mask = boss_mask * pl_mask\n",
    "fsky = np.sum(mask) * 1. / len(mask)\n",
    "\n",
    "# Apply the mask. Same for all, for now\n",
    "masked_pl = pl_lensing * mask\n",
    "masked_boss = boss * mask\n",
    "\n",
    "# Do some tidying up.\n",
    "del(boss_mask)\n",
    "del(pl_mask)\n",
    "del(pl_lensing)\n",
    "del(boss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert BOSS  n(x) -> delta n / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_boss = np.sum(masked_boss) / np.sum(mask)\n",
    "masked_boss_dn = masked_boss / mean_boss - 1.\n",
    "masked_boss_dn = mask * masked_boss_dn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cls = hp.anafast(masked_pl, map2 = masked_boss_dn, lmax = 800)\n",
    "ls = np.arange(len(Cls))\n",
    "pixwinf = hp.pixwin(Nside)[0:len(Cls)]\n",
    "# Now remove the pixel window function and deconvolve the mode mixing\n",
    "# matrix.  If the sky coverage is large, and we are using wide bins in\n",
    "# ell, then inverting the mixing matrix just reduces to dividing by\n",
    "# f_sky, the sky fraction.\n",
    "Cls = Cls / (pixwinf **2)                # Remove pixel window function\n",
    "Cls = Cls / fsky                         # Correct for f_sky\n",
    "print('Done with cross-correlation...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto correlations -- needed for the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as before!\n",
    "Clkk = hp.anafast(masked_pl, lmax = 800)\n",
    "Clgg = hp.anafast(masked_boss_dn, lmax = 800)\n",
    "Clkk = Clkk / (pixwinf **2)\n",
    "Clgg = Clgg / (pixwinf **2)\n",
    "Clkk = Clkk / fsky\n",
    "Clgg = Clgg / fsky\n",
    "print('Done with auto-correlations...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of bins and range\n",
    "Nbins = 8\n",
    "lmin = 30\n",
    "lmax = 800\n",
    "#\n",
    "bins = np.round(np.linspace(lmin, lmax, Nbins+1))   # Bin edges\n",
    "bins = bins.astype(int)\n",
    "lcenterbin = np.zeros(len(bins)-1)\n",
    "binnedCl = np.zeros(len(bins)-1)\n",
    "binnedkk = np.zeros(len(bins)-1)\n",
    "binnedgg = np.zeros(len(bins)-1)\n",
    "#\n",
    "for k in range(0, len(bins)-1):  \n",
    "    lmaxvec = np.arange(bins[k], bins[k+1], 1)\n",
    "    lcenterbin[k] = np.round(0.5 * (bins[k] + bins[k+1]))   # bin center\n",
    "    for l in lmaxvec:\n",
    "        binnedCl[k] += Cls[l]\n",
    "    binnedCl[k] = binnedCl[k] / len(lmaxvec)\n",
    "    #print(\"ell={:.1f}, Clkg={:12.4e}\".format(lcenterbin[k],binnedCl[k]))\n",
    "#\n",
    "for k in range(0, len(bins)-1): \n",
    "    lmaxvec = np.arange(bins[k], bins[k+1], 1)\n",
    "    for l in lmaxvec:\n",
    "        binnedkk[k] += Clkk[l]\n",
    "    binnedkk[k] = binnedkk[k] / len(lmaxvec)\n",
    "#\n",
    "for k in range(0, len(bins)-1): \n",
    "    lmaxvec = np.arange(bins[k], bins[k+1], 1)\n",
    "    for l in lmaxvec:\n",
    "        binnedgg[k] += Clgg[l]\n",
    "    binnedgg[k] = binnedgg[k] / len(lmaxvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing theory errors\n",
    "\n",
    "__NOTE:__ The $C_\\ell$ here are the \"empirical\" power spectra, i.e. they include noise (shot noise, reconstruction noise etc).  For convenience here we use the measured ones. It would be better to have a \"smooth\" fiducial model that fits the data.  Rather than give you yet another file we'll just use the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmavecth = np.zeros(len(bins)-1)\n",
    "for k in range(0, len(bins)-1):\n",
    "    lmaxvec = np.arange(bins[k], bins[k+1], 1)\n",
    "    for l in lmaxvec:\n",
    "        sigmavecth[k] += fsky * (2. * l + 1.) / (Clkk[l] * Clgg[l] + Cls[l]**2)\n",
    "    sigmavecth[k] = 1. / sigmavecth[k]\n",
    "sigmavecth = np.sqrt(sigmavecth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "Here we would try to fit the bias assuming some theory:\n",
    "$$\n",
    "(C_\\ell^{\\kappa g})^{\\rm measured} = b \\ (C_\\ell^{\\kappa g})^{\\rm theory}_{b = 1}\n",
    "$$\n",
    "We won't do this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We should get something pretty flat, around\n",
    "# the level of 10^{-5}ish.\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "ax.errorbar(lcenterbin, lcenterbin * binnedCl, yerr = lcenterbin * sigmavecth, fmt = 'o', label = 'Data')\n",
    "ax.set_xlabel(r'$\\ell$', fontsize = 18)\n",
    "ax.set_ylabel(r'$\\ell C_{\\ell}^{\\kappa g}$', fontsize = 18)\n",
    "ax.set_xlim([lmin, lmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we got most of the answer, this was *not* a state-of-the-art analysis!  Some things we left out:\n",
    "\n",
    "-  Use weights on galaxy catalog (completeness, stars, seeing, fiber collisions, ...) and a proper galaxy mask.\n",
    "-  We didn't apodize our masks at all, to reduce \"ringing\" in Fourier/harmonic space.  This can be especially important.\n",
    "-  We could deconvolve the mask (i.e. $M^{-1}$) (a nice place to look for more info is https://arxiv.org/abs/0705.3980, https://arxiv.org/pdf/0801.0644.pdf)\n",
    "-  Bias modeling: constant bias is probably okay on very large scales, but really not good enough for high precision work.\n",
    "-  If we were fitting, we could use the covariance matrix, including off-diagonal elements.  We could derive this from mocks:\n",
    "    -  Planck mock CMB lensing maps: https://wiki.cosmos.esa.int/planckpla2015/index.php/Simulation_data#Lensing_Simulations\n",
    "-  Convolve any theory curve with \"bin\" window function for any fits\n",
    "-  Include cosmological dependence of theory curve and jointly fit cosmological parameters\n",
    "-  Foreground biases to cross correlation (CIB, tSZ, kSZ, radio point sources)\n",
    "-  ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading ##\n",
    "\n",
    "Some place to find more information are:\n",
    "\n",
    "MASTER algorithm (https://arxiv.org/abs/astro-ph/0105302)\n",
    "Implemented in https://wwwmpa.mpa-garching.mpg.de/~komatsu/crl/list-of-routines.html http://www2.iap.fr/users/hivon/software/PolSpice/\n",
    "\n",
    "POKER algorithm (https://arxiv.org/abs/1111.0766)\n",
    "Implemented in http://www.ipag.osug.fr/~ponthien/Poker/Poker.html\n",
    "\n",
    "Optimal quadratic estimator (Matrix inversion! deal with inhomogeneous noise etc. as well, https://arxiv.org/pdf/astro-ph/9611174.pdf)\n",
    "\n",
    "For example https://github.com/dhanson/quicklens/ https://arxiv.org/abs/0705.3980\n",
    "\n",
    "Packages come and packages go, but recently the community seems to be moving towards [the NaMaster package](https://arxiv.org/abs/1809.09603) for doing power spectrum work.  This takes slightly more work to install than HealPy, which is why I didn't use it here, but once you have it the package takes care of computing the binning matrices, window function corrections, angular power spectra and (Gaussian) covariances for you!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
